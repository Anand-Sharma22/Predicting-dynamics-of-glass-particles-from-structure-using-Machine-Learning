{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting dynamics of glass particles from structure using Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "<a name = \"Fig-1\"></a>\n",
    "\n",
    "<div id = \"Fig-1\" style=\"float: right; margin-left: 20px; width: 500px; text-align: center;\">\n",
    "    <img src=\"images/image.png\" alt=\"Description\" title=\"Dynamical Heteroginity\" style=\"width: 400px;\">\n",
    "    <br>\n",
    "    <span style=\"font-size: 16px; color: K;\">Figure 1: Propensity map</span>\n",
    "   \n",
    "</div>\n",
    "A material is said to be ‘‘glassy’’ when its typical relaxation time scale becomes of the order of, and often much larger than, the typical duration of an experiment or a numerical simulation. Glass-forming liquids exhibit a significant slowing down indynamics as the temperature decreases. These glass forming liquids shows heterogensous spatial patterns of dynamics composed of mobile and immobile domains, called dynamical heterogeneity, which is another hallmark of glassy dynamics. The magnitude of dynamical heterogeneity increases with decreasing temperature, which implies the emergence of highly non-trivial correlations at lower temperatures.\n",
    "\n",
    "One can read more about the glass transition and dynamical heteroginity here: https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.83.587. \n",
    "\n",
    "One can notice the Mobile (red) and immobile (blue) region in the image as shwon in the [right](#Dynamical-Heteroginity) at relaxation timescale ($\\tau_\\alpha$). \n",
    "\n",
    "Such heterogeneous dynamical patterns are statistically reproducible if one looks at the mobility or dynamic propensity of each particle obtained from an ensemble of many independent trajectories starting from a given static configuration, called the iso-configurational ensemble. Such numerical observations suggest the existence\n",
    "of a relationship between the initial structure and future dynamics. \n",
    "\n",
    "A lot of machine learning techinques are applied to reveal the structure-dynamics relationship from another angle. Both supervised learning (where both structural and dynamical data is used in training) and unsupervised learning (where we use only structural data to train the model) methods are used to forecast the dynamics of unseen configuration. \n",
    "\n",
    "A few of recent work can be found here: \n",
    "\n",
    "- https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.130.238202\n",
    "- https://arxiv.org/pdf/2311.14752\n",
    "- https://pubs.aip.org/aip/jcp/article/156/20/204503/2841439\n",
    "- https://pubs.aip.org/aip/jcp/article/157/20/204503/2842313\n",
    "- https://www.frontiersin.org/journals/physics/articles/10.3389/fphy.2022.1007861/full\n",
    "\n",
    "In this document we are going to apply some simple machine learning techniques like Linear regression and low level neural network to predict the dynamics of glass particles. We used Monte-Carlo simulation with 3 particles and are studying the dynamics at $\\tau_\\alpha$. This document provide basic toolkit to get start with this problem of studying stuructural-dynamics relationship of glassy particles (or similar system) using machine learning. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity\n",
    "\n",
    "<b name = \"Iso-configurational\"></b>\n",
    "<div id = \"Fig-2\" style=\"float: right; margin-left: 20px; width: 500px; text-align: center;\">\n",
    "    <img src=\"images/image1.png\" alt=\"Description\" title=\"iso-configurational ensemble\" style=\"width: 400px;\">\n",
    "    <br>\n",
    "    <span style=\"font-size: 16px; color: K;\">Figure 2: iso-configurational ensemble</span>\n",
    "    <br>\n",
    "    <span style=\"font-size: 12px; color: #555;\"> <a href=\"https://gmshajahan.wixsite.com/website/single-post/2023/01/01/theory-of-the-isoconfigurational-ensemble\" target=\"_blank\">Source</a></span>\n",
    "</div>\n",
    "\n",
    "To see heterogeneous dynamics in real space and assess the influence of static structure, we use the iso-configurational ensemble. The isoconfigurational ensemble is a set of different motion trajectories of the same initial structure, but with different initial momenta. (Different random seed for Monte-Carlo).\n",
    "\n",
    "In particular, we compute a propensity parameter, $p_i$, which is given by\n",
    "\n",
    "$$\n",
    "    p_i(t) = \\left\\langle |\\Delta {\\bf r}^{\\rm CR}_i(t)| \\right\\rangle_{\\rm iso}, \n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot \\rangle_{\\rm iso}$ is average over different trajectories starting from a given static configuration. When we plot this propensity map on the initial position of particles, we get map as shown in [Fig-1](#Fig-1). Regions with larger and smaller values of $p_i$ correspond to mobile and immobile regions. \n",
    "\n",
    "We also consider an another propensity parameter $q_i(t)$ based on the bond-breaking approach~\\cite{yamamoto1998dynamics,shiba2012relationship}.\n",
    " $q_i(t)$ is defined by\n",
    "$$\n",
    "    q_i(t) = \\left\\langle n_i(t)/n_i(0) \\right\\rangle_{\\rm iso} ,\n",
    "$$\n",
    "where $n_i(0)$ is the\n",
    "number of neighbor particles within a cutoff radius $1.4\\sigma_{\\alpha \\beta}$\n",
    "of the particle $i$ and $n_i(t)$ is the number of particles that were initially part of the neighbors (at $t=0$) and are still inside a cutoff $1.8\\sigma_{\\alpha \\beta}$ at time $t$.\n",
    "\n",
    "These two can serve as target functions (Y) for our machine learning models. In this document we are mainly going to focus on propensity parameter $p_i(t)$ as our target function but one can also use $q_i(t)$. \n",
    "\n",
    "\\$$\n",
    "{\\bf Y}_i = p_i(\\tau_\\alpha)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Descripotrs \n",
    "\n",
    "We employ the Behler-Parrinello (BP) structure descriptors (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.98.146401), which has been widely used in various problems including the prediction of glassy dynamics in two-dimensions (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.114.108001).\n",
    "The BP descriptors composed of the radial descriptor $G$ and angular descriptor $\\Psi$. \n",
    "\n",
    "We define sets of particles, S, M, and L, which contain only small (S), medium (M), and large (L) particles, respectively. We also introduce labels $A, B$ (= S, M, L) to denote species for the particles $j$ and $k$, respectively.\n",
    "{\\bf In our codes, S is species=1, M is species=2, and L is species=3.}\n",
    "\n",
    "The radial descriptor $G_i^A$ for each particle $i$ is defined by\n",
    "$$\n",
    "    G_i^A = {\\sum_{j \\in A}}' e^{-(r_{ij}-\\mu)^2/\\ell^2} f_c(r_{ij}) ,\n",
    "$$\n",
    "where $r_{ij}=|{\\bf r}_i-{\\bf r}_j|$ is the distance between two particles ($i$ and $j$), and $\\mu$ and $\\ell$ are parameters.\n",
    "$\\sum'$ denotes that the particle $i$ is removed from the summation.\n",
    "$f_c(r)$ is the cut-off function, which is defined by $f_c(r)=\\frac{1}{2}\\left[ \\cos(\\pi r/R_c) + 1\\right]$ for $r \\leq R_c$ and $f_c(r)=0$ for $r>R_c$~\\cite{behler2015constructing}. $R_c$ is a cut-off radius and we set $R_c=5.0 \\sigma_{\\rm LS}$.\n",
    "We vary $\\mu$ between $0.3 \\sigma_{\\rm LS}$ and $5.0 \\sigma_{\\rm LS}$ in increments of $0.1\\sigma_{\\rm LS}$ with $\\ell=0.1\\sigma_{\\rm LS}$.\n",
    "Thus, for each particle, we have $144(=3 \\times 48)$ different functions parameterized by $\\mu$ (with a fixed $\\ell$).\n",
    "We note that the summation runs over only the particles of a specific species $A$(=S, M, L).\n",
    "\n",
    "The angular descriptor $\\Psi_i^{AB}$ is defined by\n",
    "$$\n",
    "\\Psi_i^{AB} = 2^{1-\\zeta} {\\sum_{\\substack{{j \\in A}, \\ {k \\in B}\n",
    "(j \\neq k)}}}' e^{-(r_{ij}^2+r_{ik}^2+r_{jk}^2)/\\xi^2} \\nonumber \n",
    "\\times (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times f_c(r_{ij}) f_c(r_{ik}) f_c(r_{jk}) ,\n",
    "$$\n",
    "where $\\theta_{ijk}$ is the angle at the corner $i$ of the triangle defined by particles $i$, $j$, and $k$, and $\\xi$, $\\lambda$, and $\\zeta$ are parameters varied systematically.\n",
    "We employ $22$ sets of the parameters ($\\xi$, $\\lambda$, $\\zeta$) (reported in https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.114.108001) in unit of $\\sigma_{\\rm LS}$.\n",
    "Thus we have $132 (=6 \\times 22)$ functions.\n",
    "\n",
    "Thus, in total, we have $M=276(=144+132)$ functions for each particle.\n",
    "These functions constitute a feature vector for a particle $i$, given by\n",
    "$$\n",
    "    {\\bf X}_i=\\left(X_i^{(1)}, X_i^{(2)}, \\cdots, X_i^{(M)}\\right) .\n",
    "$$\n",
    "This is the input for training model in Linear Regression and Neural Network.\n",
    "\n",
    "In particular, we will store different structural descriptors in ${\\bf X}_i$ as follows: \n",
    "\n",
    "$$\n",
    "    {\\bf X}_i=\\left( G_i^{\\rm S}, G_i^{\\rm M}, G_i^{\\rm L}, \\Psi_i^{\\rm SS}, \\Psi_i^{\\rm SM}, \\Psi_i^{\\rm SL}, \\Psi_i^{\\rm MM}, \\Psi_i^{\\rm ML}, \\Psi_i^{\\rm LL} \\right) .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summery of the data\n",
    "\n",
    "We calculated the 30 trajectories of iso-configurational ensemble. We calculated \n",
    "$$\n",
    "    p_i(t) = \\left\\langle |\\Delta {\\bf r}^{\\rm CR}_i(t)| \\right\\rangle_{\\rm iso}, \n",
    "$$\n",
    "\n",
    "$$\n",
    "    q_i(t) = \\left\\langle n_i(t)/n_i(0) \\right\\rangle_{\\rm iso} ,\n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot \\rangle_{\\rm iso}$ is average over these 30 trajectories starting from a given static configuration. \n",
    "\n",
    "We used 10 such configurtions containing 4000 particles in each configuration. Out of those 4000, 1760 were small type, 800 were medium type, and 1440 were large type. \n",
    "\n",
    "The data formate in the files is as following: \n",
    "$$\n",
    "    {\\bf R}_i=\\left(x,y,type, p_i(\\tau_\\alpha),q_i(\\tau_\\alpha), G_i^{\\rm S}, G_i^{\\rm M}, G_i^{\\rm L}, \\Psi_i^{\\rm SS}, \\Psi_i^{\\rm SM}, \\Psi_i^{\\rm SL}, \\Psi_i^{\\rm MM}, \\Psi_i^{\\rm ML}, \\Psi_i^{\\rm LL} \\right) .\n",
    "$$\n",
    "\n",
    "where, $x$ is x-coordinate, $y$ is y-coordinate , $type$ is particle type (1 for small, 2 for medium, 3 for large) , and rest of definations are same. So, each row is a vector of length 281, with first five as $x,y,type,p_i(\\tau_\\alpha),q_i(\\tau_\\alpha)$ , and remaining 276 as ${\\bf X}_i$ given above to be used as features for training. We used the same sequence in indexing as given above. \n",
    "\n",
    "Now we have 10 such configurations. So we will use 9 configurations for training and 1 unseen configuration for testing. (One can also use some for validation for neural networks). \n",
    "We will do it iteratively, like take 1 as test and 9 as training and do it for each configuration so basically 10 times for better statistics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code given below is to open a file. You can change the path and also structure of this function as per your need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_matrix_full_diff_species(ps,temperature,timescale,cnf):  \n",
    "    with open(f\"outputifile(mobility_map_FL2D_C3_Ps{ps}_N4000_Nalpha800_T{temperature}00_delta_max0.120_cal_stop{timescale}_Cnf{cnf},index).txt\", 'r') as file:\n",
    "        first_line = file.readline().strip() # read the first line of the file\n",
    "        num_particles, box_length = map(float, first_line.split()[:2]) # first two numbers in the first line of the file which is information about Box Length and Num_particles\n",
    "        data_matrix = np.loadtxt(file, dtype=float) # read the rest of the file into a numpy array\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to import all 10 configuration as one pandas file with dimension 40000X281. We are using DataFrame as it's easy to do the analysis particle wise or more specific statistics. Since it's an introductory toolkit, we will not train particle wise, but in the community that is a standard way to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnf in range(1,11):\n",
    "    if cnf == 1:\n",
    "        data_ =data_matrix_full_diff_species(\"0.00\",\"0.30\",4000000,cnf)\n",
    "        data = pd.DataFrame(data_)\n",
    "    else:\n",
    "        data_ =data_matrix_full_diff_species(\"0.00\",\"0.30\",4000000,cnf)\n",
    "        data = pd.concat([data,pd.DataFrame(data_)],axis=0)\n",
    "\n",
    "data.reset_index(drop=True,inplace=True) # reset the index of the dataframe after concatenation so that each configuration is easy to call. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling (Normalization)\n",
    "\n",
    "The goal of scaling is to transform features to be on a similar scale. For example, consider the following two features:\n",
    "\n",
    "Feature X spans the range 1500 to 2000000.\n",
    "\n",
    "Feature Y spans the range 5 to 15.\n",
    "\n",
    "<b name = \"Standard-Scaling\"></b>\n",
    "<div id = \"Fig-3\" style=\"float: right; margin-left: 20px; width: 500px; text-align: center;\">\n",
    "    <img src=\"images/image2.png\" alt=\"Description\" title=\"Before-After Standard Scaling\" style=\"width: 400px;\">\n",
    "    <br>\n",
    "    <span style=\"font-size: 16px; color: K;\">Figure 3: Before and After standard normalization</span>\n",
    "    <br>\n",
    "    <span style=\"font-size: 12px; color: #555;\"> <a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Frole-feature-scaling-machine-learning-deepak-kumar-1c&psig=AOvVaw3OrFuGLIHbdGkQZzTmw4c0&ust=1731058296143000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCKDcn-f0yYkDFQAAAAAdAAAAABAa\" target=\"_blank\">Source</a></span>\n",
    "</div>\n",
    "\n",
    "\n",
    "These two features span very different ranges. Scaling might manipulate X and Y so that they span a similar range, perhaps 0 to 1.\n",
    "\n",
    "Also, uou can think of examples like different features are in different units and the machine know nothing about them.\n",
    "**You want to normalize the features to ensure that all features contribute equally to the model**. It become more important with distance based algorithm like KNN or recently discovered [Information Imbalance](https://academic.oup.com/pnasnexus/article/1/2/pgac039/6568571).  \n",
    "\n",
    "You can read more about importance of normalization of and various types of Normalization schemes here: https://developers.google.com/machine-learning/crash-course/numerical-data/normalization\n",
    "\n",
    "\n",
    "\n",
    "There are many normalization methods, we are going to use what is called **Standard-Scaling** where we scale each feature such that it's mean $\\mu_j$ is 0 and standard deviation $\\sigma_j$ is 1. \n",
    "\n",
    "For this we scale each element of the feature by: \n",
    "\n",
    "$$\n",
    "X_i^{\\rm j} = (X_i^{\\rm j} - \\mu^{\\rm j})/\\sigma^{\\rm j}\n",
    "$$\n",
    "\n",
    "for some feature j where $\\mu^{\\rm j}$ and $\\sigma^{\\rm j}$ are it's mean and standard deviation. \n",
    "\n",
    "### Some more stretigies \n",
    "We usually do not have access of test data while training our model. So we will calculate $\\mu^{\\rm j}$ and $\\sigma^{\\rm j}$ for training data only. When we get the test data we usually normalize it by using the same $\\mu^{\\rm j}$ and $\\sigma^{\\rm j}$ calculated from training data instead of calculating for it seperately and then normalizing. One reason for this can be that the training data set it quiet big as compared to test data set is more close to true population mean. \n",
    "\n",
    "Also, we usually normalize features and not target. \n",
    "\n",
    "In python we have inbuilt library for it which we will be using: \n",
    "https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are not segerating the data into training and testing data but one should segerate the training data for fitting the standard-scalar and then use this fitted model to transform both training and Test data. \n",
    "\n",
    "Here XX is set of 276 input features. \n",
    "scaler.fit_transform is first fitting the StandardScaler model i.e calculating $\\mu^{\\rm j}$ and $\\sigma^{\\rm j}$ and then transforming the individual data points $X_i^{\\rm j}$. \n",
    "for test data we will just use scaler.transform as it is already fitted with training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "XX = data.iloc[:,5:]\n",
    "YY = data.iloc[:,3]\n",
    "\n",
    "XX_norm = scaler.fit_transform(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "It is a statistical method used to model the relationship between a dependent variable (response variable) and one or more independent variables (predictors). The goal is to find the best-fitting straight line (the regression line) through the data points that minimizes the sum of the squared differences between the observed values and the values predicted by the line.\n",
    "\n",
    "In linear regression, the relationship between the dependent variable  y  and the independent variables  X  is modeled using a linear equation:\n",
    "\n",
    "$$\n",
    " y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon \n",
    "$$\n",
    "\n",
    "where, \n",
    "- $y$  is the dependent variable.\n",
    "- $\\beta_0$  is the y-intercept (constant term).\n",
    "- $\\beta_1$, $\\beta_2$, $\\ldots$, $\\beta_p$  are the coefficients for the independent variables  $x_1$, $x_2$, $\\ldots$, $x_p$ .\n",
    "- $\\epsilon$  is the error term, representing the difference between the observed and predicted values.\n",
    "\n",
    "It can be written in matrix form as:\n",
    "\n",
    "$$\n",
    " \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \n",
    "$$\n",
    "where:\n",
    "\n",
    "- $\\mathbf{y}$ is the $n \\times 1$ vector of the dependent variable.\n",
    "- $\\mathbf{X}$ is the $n \\times (p+1)$ matrix of independent variables (including a column of ones for the intercept).\n",
    "- $\\boldsymbol{\\beta}$ is the $(p+1) \\times 1$ vector of coefficients.\n",
    "- $\\boldsymbol{\\epsilon}$ is the $n \\times 1$ vector of errors.\n",
    "\n",
    "\n",
    "The coefficients can be calculated using the normal equation:\n",
    "\n",
    "$$\n",
    " \\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} \n",
    "$$\n",
    "\n",
    "To know more about linear regression and check the detailed derivation of normal equation, one can look at: https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf\n",
    "\n",
    "\n",
    "In python we have inbuilt linear regression module: https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "We are going to use it. One can calculate all the things like coffiecients, intercepts, mean square errors etc. (check the documentation). We are only going to use it for prediction of test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's correlation cofficient\n",
    "\n",
    "<b name = \"Pearson\"></b>\n",
    "<div id = \"Fig-4\" style=\"float: right; margin-left: 20px; width: 500px; text-align: center;\">\n",
    "    <img src=\"images/image3.png\" alt=\"Description\" title=\"Pearson's Cofficient\" style=\"width: 400px;\">\n",
    "    <br>\n",
    "    <span style=\"font-size: 16px; color: K;\">Figure 4: Pearson's correlation cofficient</span>\n",
    "    <br>\n",
    "    <span style=\"font-size: 12px; color: #555;\"> <a href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\" target=\"_blank\">Source</a></span>\n",
    "</div>\n",
    "\n",
    "How to quantify our prediction ? \n",
    "\n",
    "There are many ways like calculating mean square error, $R^2$, etc. Pearson's cofficient it one such method which calculate how the well the prediction and ground truth values are correleted. More formally: \n",
    "\n",
    "[Pearson’s correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), often denoted as  $\\rho_{x,y}$ , is a measure of the linear relationship between two variables $x$ and $y$. It quantifies the degree to which two variables are linearly related. The value of  $\\rho$  ranges from -1 to 1, where:\n",
    "\n",
    "- $\\rho_{x,y}$ = 1  indicates a perfect positive linear relationship,\n",
    "- $\\rho_{x,y}$ = -1  indicates a perfect negative linear relationship,\n",
    "- $\\rho_{x,y}$ = 0  indicates no linear relationship.\n",
    "\n",
    "The formula for Pearson’s correlation coefficient is:\n",
    "\n",
    "$$\n",
    " \\rho_{x,y} = \\frac{\\sum (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum (x_i - \\overline{x})^2 \\sum (y_i - \\overline{y})^2}} \n",
    "$$\n",
    "\n",
    "where it is summed over all the sample points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution: \n",
    "\n",
    "We are going to apply linear regression 10 times, such that one configuration is used as test data each time. \n",
    "So, the loop iteratively selecting one cnf as test and remaining as training data. \n",
    "\n",
    "Here is what we are doing inside the loop: \n",
    "1. Collecting the test data and saving it as X_test and y_test\n",
    "2. Saving all the remaining data as training data. \n",
    "3. Fitting the StandardScaler() with the training data, and transfer the training and test data using this fitted StandardScaler().\n",
    "4. Calling the LinearRegression Function and fitting it with the training data. \n",
    "5. Once fitted, it calculate the cofficients $\\beta$ and slope $\\beta_o$ \n",
    "6. We use this fitted model to predict the propensity of test dataset. \n",
    "7. To quantify the prediction, we calculate the pearson's cofficient. \n",
    "8. We store that in the list **pearson**. \n",
    "\n",
    "The **pearson** list contain pearson's cofficient for all the 10 cnfs as test data. We can average the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pearson Correlation Coefficient: 0.6903249722609409\n"
     ]
    }
   ],
   "source": [
    "pearson = []\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    X_test = data.iloc[(cnf-1)*4000:cnf*4000,5:]\n",
    "    y_test = data.iloc[(cnf-1)*4000:cnf*4000,3]\n",
    "\n",
    "    X_train = data.drop(data.index[(cnf-1)*4000:cnf*4000]).iloc[:,5:]\n",
    "    y_train = data.drop(data.index[(cnf-1)*4000:cnf*4000]).iloc[:,3]\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    linear = LinearRegression()\n",
    "    linear.fit(X_train_norm, y_train)\n",
    "\n",
    "    # Predict the mobility\n",
    "    y_pred = linear.predict(X_test_norm) # Predict the mobility\n",
    "    pearson = np.append(pearson,pearsonr(y_test,y_pred)[0])\n",
    "    \n",
    "    \n",
    "pearson = np.array(pearson)\n",
    "print(f\"Mean Pearson Correlation Coefficient: {pearson.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "<b name = \"Neural-Network\"></b>\n",
    "<div id = \"Fig-5\" style=\"float: right; margin-left: 20px; width: 500px; text-align: center;\">\n",
    "    <img src=\"images/image4.png\" alt=\"Description\" title=\"Neural Network\" style=\"width: 400px;\">\n",
    "    <br>\n",
    "    <span style=\"font-size: 16px; color: K;\">Figure 5: Sketch of Neural Network</span>\n",
    "    <br>\n",
    "    <span style=\"font-size: 12px; color: #555;\"> <a href=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fdesigning-your-neural-networks-a5e4617027ed&psig=AOvVaw1R1bzAW5IrJYr9sXv2ZyWj&ust=1731076172695000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCNi4u7S3yokDFQAAAAAdAAAAABAb\" target=\"_blank\">Source</a></span>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "It's a broad topic and it's impossible to cover it here in enough detail. One can follow the below material to get decently familiar with neural Networks: \n",
    "- http://neuralnetworksanddeeplearning.com/ which provide very intutive introduction about neural networks \n",
    "- Set of 5 courses by Andrew ng on Deep learning which is more on the application side: \n",
    "    - Course 1: https://youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&si=lp05oX7xLk_mxIy0\n",
    "    - Course 2: https://youtube.com/playlist?list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&si=DnxDiNZ6SLKJi7PO\n",
    "    - Course 3: https://youtube.com/playlist?list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&si=O7wW7lUHEXBi7SOT\n",
    "    - Course 4: https://youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&si=4wfz93-VquezAfm2\n",
    "    - Course 5: https://youtube.com/playlist?list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6&si=iH3wZcNw2GxW1itA\n",
    "- A concise introduction can be found here: \n",
    "    - https://www.ibm.com/topics/neural-networks#:~:text=IBM-,What%20is%20a%20neural%20network%3F,options%20and%20arrive%20at%20conclusions.\n",
    "    - https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\n",
    "\n",
    "\n",
    "We are going to use Multilayer Perceptron Regressor. I quick summery about it can be give as: \n",
    "\n",
    "Multilayer Perceptron (MLP) Regression is a type of neural network model used for regression tasks, where the goal is to predict a continuous output variable. Unlike linear regression, which models the relationship between input and output using a linear function, MLP regression can model complex, non-linear relationships\n",
    "\n",
    "An MLP consists of multiple layers of neurons, each layer connected to the next through weighted connections. The typical structure of an MLP includes an input layer, one or more hidden layers, and an output layer. Each neuron in a layer applies a weighted sum of its inputs, passes the result through an activation function, and forwards the output to the next layer. \n",
    "\n",
    "It consist of: \n",
    "1. Input Layer: Takes the input features  $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$ .\n",
    "2.\tHidden Layers: One or more layers of neurons that perform non-linear transformations of the inputs.\n",
    "3.\tOutput Layer: Produces the final prediction.\n",
    "\n",
    "We are gping to use MLP Regressor of scikit-learn: https://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPRegressor.html \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper paramete tuning\n",
    "\n",
    "Hyperparameters are configuration settings used to control the learning process of neural networks. Unlike model parameters (such as weights and biases) which are learned during training, hyperparameters are set before the training process begins. Examples of hyperparameters include the learning rate, number of hidden layers, number of neurons per layer, activation functions, batch size, number of epochs, and regularization techniques like dropout rates. These settings significantly influence the performance and efficiency of the neural network.\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters that yield the best performance for a neural network on a given task. Proper tuning is crucial because the choice of hyperparameters can drastically affect the model’s ability to learn and generalize from the data. Poorly chosen hyperparameters may lead to issues such as underfitting, where the model is too simple to capture the underlying patterns, or overfitting, where the model learns the noise in the training data and performs poorly on unseen data.\n",
    "\n",
    "Here we are going to use GridSearchCV to search Hyper-parameter by giving a grid space. It's an inbuilt module in python and you can check it's documentation here: https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "A brief summery of Hyper-parameter tuning can be found here: https://www.run.ai/guides/hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   5.7s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   8.1s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  11.5s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  11.7s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  11.8s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  12.1s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  16.4s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   9.1s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  12.8s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=  21.3s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  11.1s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  13.3s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  14.3s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  10.2s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  18.7s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   8.3s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  13.9s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  20.1s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  19.6s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  14.3s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  16.2s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  11.8s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  16.2s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  15.3s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   6.7s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  12.7s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  13.6s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   6.8s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  14.4s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  14.1s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   7.5s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  23.1s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   4.8s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   7.0s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   9.5s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   4.6s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   7.0s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   5.4s\n",
      "[CV] END activation=relu, alpha=0.01, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  22.6s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   8.5s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   6.9s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   6.9s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   8.2s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  11.2s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  10.9s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   9.2s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   9.2s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  11.8s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=  11.8s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   9.4s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   9.8s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.4s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.4s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   9.0s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.9s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   9.0s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.9s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   4.2s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.2s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.2s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  13.2s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   4.6s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=  13.0s\n",
      "[CV] END activation=relu, alpha=0.1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   9.6s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.3s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   4.5s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   4.3s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.8s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   6.5s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.1s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.1s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   6.9s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   7.2s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   8.5s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   7.1s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   7.1s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.3s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.2s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.0s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.1s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   7.0s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.3s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=1, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   6.8s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.3s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=2, learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.9s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.8s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.9s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.8s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.8s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.9s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2), learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   4.2s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.8s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.8s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   4.1s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.7s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.6s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.4s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   3.1s\n",
      "[CV] END activation=relu, alpha=10, batch_size=50, hidden_layer_sizes=(2, 2, 2), learning_rate=constant, solver=adam; total time=   2.4s\n",
      "MLPRegressor(alpha=0.01, batch_size=50, early_stopping=True,\n",
      "             hidden_layer_sizes=(2, 2), max_iter=10000, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are using the whole data set because we are going to do this experiment 10 times, so doing the GridSearchCV 10 times would be computationally expensive. \n",
    "But one should use only training data for GridSearchCV.\n",
    "Here is a small set of hyperparameters to search over. But in reality, one should search over a larger set of hyperparameters.\n",
    "\"\"\"\n",
    "\n",
    "# Define the MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=10000 , early_stopping = True , random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(2),(2,2), (2,2,2)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'learning_rate': ['constant'],\n",
    "    'alpha' : [0.01,0.1,1,10],\n",
    "    \"batch_size\" : [50]\n",
    "}\n",
    "np.random.seed(42)\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=10, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(XX_norm, YY)\n",
    "\n",
    "# Return the best estimator\n",
    "best_mlp = grid_search.best_estimator_\n",
    "print(best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this best architecture we get in our final model to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Pearson Correlation Coefficient: 0.6795204415023716\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Doing the same steps as we did in Linear regression just with neural network this time\n",
    "\"\"\"\n",
    "\n",
    "pearson = []\n",
    "\n",
    "for cnf in range(1,11):\n",
    "    X_test = data.iloc[(cnf-1)*4000:cnf*4000,5:]\n",
    "    y_test = data.iloc[(cnf-1)*4000:cnf*4000,3]\n",
    "\n",
    "    X_train = data.drop(data.index[(cnf-1)*4000:cnf*4000]).iloc[:,5:]\n",
    "    y_train = data.drop(data.index[(cnf-1)*4000:cnf*4000]).iloc[:,3]\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    #Adjust the parameters according to the best estimator\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(2,2), activation='relu', solver='adam', learning_rate='constant', alpha=0.01, batch_size=50, max_iter=10000, early_stopping=True, random_state=42)\n",
    "    mlp.fit(X_train_norm, y_train)\n",
    "\n",
    "    # Predict the mobility\n",
    "    y_pred = mlp.predict(X_test_norm) # Predict the mobility\n",
    "    pearson = np.append(pearson,pearsonr(y_test,y_pred)[0])\n",
    "\n",
    "pearson = np.array(pearson)\n",
    "print(f\"Mean Pearson Correlation Coefficient: {pearson.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
